{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize\n",
    "import math\n",
    "import quantopian.research\n",
    "from quantopian.pipeline.data import Fundamentals\n",
    "from quantopian.pipeline import Pipeline, CustomFilter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mean(W, R):\n",
    "    \"\"\"\n",
    "    Computes the expected return of a portfolio \n",
    "    Requires: List of asset weights and list of asset expected returns\n",
    "    \"\"\"\n",
    "    mean=np.sum(W*R)\n",
    "    return mean\n",
    "\n",
    "def compute_var(W, C):\n",
    "    \"\"\"\n",
    "    Computes the variance of a portfolio \n",
    "    Requires: List of asset weights and asset covariance matrix\n",
    "    \"\"\"\n",
    "    var= np.dot(np.dot(W,C), W) \n",
    "    return var\n",
    "\n",
    "def mean_var(W, R, C):\n",
    "    \"\"\"\n",
    "    Calculates portfolio expected return and variance\n",
    "    Requires: List of weights, list of asset expected returns, and asset covariance matrix.\n",
    "    \"\"\"\n",
    "    mean= compute_mean(W,R)\n",
    "    var= compute_var(W,C)\n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def market_cap(price, shares_outstanding):\n",
    "    return price*shares_outstanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sharpe_ratio(W, R, C, Rf):\n",
    "    \"\"\"\n",
    "    Calculates a sharpe ratio of a portfolio\n",
    "    Requires: Portfolio weights, list of asset returns, asset covariance matrix, risk free rate\n",
    "    Sharpe ratio = (Mean portfolio return − Risk-free rate)/Standard deviation of portfolio return.\n",
    "\n",
    "    \"\"\"\n",
    "    mean, var= mean_var(W, R, C) \n",
    "    return (mean- Rf)/ np.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sharpe(mean, std, rf):\n",
    "    \"\"\"\n",
    "    Sharpe ratio = (Mean portfolio return − Risk-free rate)/Standard deviation of portfolio return.\n",
    "\n",
    "    \"\"\"\n",
    "    return (mean- rf)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w_fitness(W, R, C, Rf):\n",
    "    if sharpe_ratio(W, R, C, Rf) < 0: \n",
    "        return 1000000000000\n",
    "    else: \n",
    "        return 1/sharpe_ratio(W, R, C, Rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def solve_weights(R, C, Rf, low=0., high=1): \n",
    "    \"\"\"\n",
    "    Optimizes the list of assets using mean variance optimization, \n",
    "    Requires: expected returns, covariance matrix, risk free rate,\n",
    "    low and high for upper and lower bounds for weight\n",
    "    \"\"\"\n",
    "    n= len(R)\n",
    "    W= np.ones([n])/n\n",
    "    b_=[(low, high) for i in range(n)] \n",
    "    c_ = ({'type':'eq', 'fun': lambda W: sum(W)-1. }) \n",
    "    optimized = scipy.optimize.minimize(w_fitness, W, (R, C, Rf), method='SLSQP', constraints=c_, bounds=b_)\n",
    "    if not optimized.success: #copied\n",
    "        raise BaseException(optimized.message)\n",
    "    return optimized.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def solve_frontier(R, C, RF):\n",
    "    def fitness(W, R, C, r):\n",
    "        mean, var= mean_var(W, R, C)\n",
    "        penalty= 100*abs(mean-r)\n",
    "        return var+ penalty\n",
    "    frontier_mean= []\n",
    "    frontier_var= []\n",
    "    n= len(R)\n",
    "    for r in np.linspace(min(R), max (R), num= 20):\n",
    "        W = np.ones([n]) / n  # start optimization with equal weights\n",
    "        b_ = [(0, 1) for i in range(n)]\n",
    "        c_ = ({'type': 'eq', 'fun': lambda W: np.sum(W) - 1.})\n",
    "        optimized = scipy.optimize.minimize(fitness, W, (R, C, r), method='SLSQP', constraints=c_, bounds=b_)\n",
    "        if not optimized.success:\n",
    "            raise BaseException(optimized.message)\n",
    "        frontier_mean.append(r)\n",
    "        frontier_var.append(compute_var(optimized.x, C))\n",
    "    return np.array(frontier_mean), np.array(frontier_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_frontier(R, C, rf):\n",
    "    W = solve_weights(R, C, rf)\n",
    "    tan_mean, tan_var = mean_var(W, R, C)  # calculate tangency portfolio\n",
    "    front_mean, front_var = solve_frontier(R, C, rf)  # calculate efficient frontier\n",
    "    result= {'weights': W, 'tangent_mean': tan_mean,'tangent_variance': tan_var}\n",
    "    # Weights, Tangency portfolio asset means and variances\n",
    "    return result, front_mean, front_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assetmeanvar(returns):  \n",
    "    \"\"\"\n",
    "    Calculates the expected return and covariance matrix of the dataframe. \n",
    "    Requires: the returns dataframe.\n",
    "    \"\"\"\n",
    "    covars=np.cov(np.matrix(returns).T)*250\n",
    "    (rows, cols)= np.shape(returns)\n",
    "    expectedreturns= np.array([])\n",
    "    for i in range(cols):\n",
    "        expectedreturns=np.append(expectedreturns,(1+np.mean(returns.iloc[:,i]))**250-1)\n",
    "    return covars, expectedreturns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weightsbymarketcap(returns, market_cap): \n",
    "    \"\"\"\n",
    "    Calculates the weights by market cap. \n",
    "    Requires: returns dataframe and the assets' respective market cap.\n",
    "    \"\"\"\n",
    "    W=np.array([])\n",
    "    total=0\n",
    "    for column in returns.columns:\n",
    "        total+= market_cap[column]\n",
    "    for column in returns.columns:\n",
    "        W=np.append(W, market_cap[column]/total)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def views_and_links(views, markcap, returns_columns):\n",
    "    \"\"\"\n",
    "    Takes in a list of views to create a views vector. \n",
    "    Creates a links matrix that matches with the views vector, while accounting for market cap weights.\n",
    "    Requires: list of views, dictionary of assets and their market caps, returns dataframe's column names\n",
    "    \"\"\"\n",
    "    row, col = len(views), len(returns_columns)\n",
    "    location= []\n",
    "    Q= []\n",
    "    for i in range(row): #finding the index for the > and < signs to divide the list between the greater and lesser sides.\n",
    "        Q.append(views[i][-1])\n",
    "        try:\n",
    "            location.append(views[i].index('>'))\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            location.append(views[i].index('<'))\n",
    "        except Exception:\n",
    "            pass\n",
    "    P = np.zeros([row, col])\n",
    "    returnsindex=dict()\n",
    "    for i, n in enumerate(returns_columns): #used to match the asset\n",
    "        returnsindex[n]= i\n",
    "    for i, v in enumerate(views):\n",
    "        front=[]\n",
    "        back=[]\n",
    "        ftotal=0\n",
    "        btotal=0\n",
    "        for z in range(location[i]):\n",
    "            x=views[i][z]\n",
    "            front.append(x)\n",
    "            ftotal+=markcap[x]\n",
    "        for name in front:\n",
    "            if views[i][location[i]]=='>': # match the link with the column index of the asset\n",
    "                P[i, returnsindex[name]]= markcap[name]/ftotal #accounting for market cap weight on the greater side\n",
    "            else:\n",
    "                P[i, returnsindex[name]]= (markcap[name]*-1)/ftotal#accounting for market cap weight on the lesser side\n",
    "        for z in range(location[i]+1,len(views[i])-1):\n",
    "            x=views[i][z]\n",
    "            back.append(x)\n",
    "            btotal+=markcap[x]\n",
    "        for name in back:\n",
    "            if views[i][location[i]]=='<':\n",
    "                P[i, returnsindex[name]]= markcap[name]/btotal #accounting for market cap weight on the greater side\n",
    "            else:\n",
    "                P[i, returnsindex[name]]= (markcap[name]*-1)/btotal#accounting for market cap weight on the lesser side\n",
    "    return np.array(Q), P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uncertainty_m(tau, C, P):\n",
    "    \"\"\"\n",
    "    Creates a diagonal covariance matrix of error terms from the expressed views representing the uncertainty in each view (K x K matrix). \n",
    "    Requires: tau, covariance matrix, and views' links matrix\n",
    "    \"\"\"\n",
    "    w = np.dot(np.dot(np.dot(tau, P),C),P.T)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eqexcessret(tau, C, P, w, Q, eqRet):\n",
    "    \"\"\"\n",
    "    Combines the different components to return the combined return vector\n",
    "    Requires: tau, covariance matrix, views' links matrix, uncertainty matrix, views vector, equilibrium expected returns\n",
    "    \"\"\"\n",
    "    sub_a = np.linalg.inv(np.dot(tau, C))\n",
    "    sub_b = np.dot(np.dot(np.transpose(P), np.linalg.inv(w)), P)\n",
    "    sub_c = np.dot(np.linalg.inv(np.dot(tau, C)), eqRet)\n",
    "    sub_d = np.dot(np.dot(np.transpose(P), np.linalg.inv(w)), Q)\n",
    "    eqRet_adj = np.dot(np.linalg.inv(sub_a + sub_b), (sub_c + sub_d))\n",
    "    return eqRet_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_returns_data(symbols, start, end): \n",
    "    \"\"\"\n",
    "    Takes in the symbols for the assets to load pricing data and converts into daily returns, \n",
    "    Requires: asset symbols as a list, starting and ending dates \n",
    "    \"\"\"\n",
    "    data={}\n",
    "    for symbol in symbols:\n",
    "        data[symbol]=get_pricing(symbol, start_date=start, end_date=end, symbol_reference_date=None, frequency='daily', fields='price', handle_missing='raise', start_offset=0)\n",
    "    returns={}\n",
    "    for symbol in symbols:\n",
    "        returns[symbol] = data[symbol].pct_change()[1:]\n",
    "    returns_df=pd.DataFrame.from_dict(returns, orient='columns', dtype=None)\n",
    "    returns_df=returns_df[symbols]\n",
    "    return returns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expectedreturn(symbols,returns_df):\n",
    "    expectedreturn= np.array([])\n",
    "    for symbol in symbols:\n",
    "        expectedreturn=np.append(expectedreturn,(1+np.mean(returns_df[column]))**250-1)\n",
    "    return expectedreturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lam(m, v, Rf): \n",
    "    \"\"\"\n",
    "    Calculates the risk aversion coefficient\n",
    "    Requires: expected (market/ benchmark) return, \n",
    "    variance of the (market/benchmark) returns, \n",
    "    and risk free rate\n",
    "    \"\"\"\n",
    "    lam= (m-Rf)/v\n",
    "    return lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eqret(C, W, lam):\n",
    "    \"\"\"\n",
    "    Calculates the implied equilibrium excess return by finding the dot products of lam, covariance matrix, and market weights\n",
    "    Requires: Covariance matrix, market weights, and lambda\n",
    "    \"\"\"\n",
    "    eqret= np.dot(np.dot(lam,C),W)\n",
    "    return eqret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For years, the world has been excited about the opportunities of artificial intelligence in the field of finance. Largely, it’s because financial datasets are inherently large and complex making predicting investment trends near impossible without the assistance of programs. However, an easier problem to solve is the allocation of assets in an investment portfolios. Companies like Wealthfront have been touting investment robots that can help allocate investments using key investment principles like CAPM, Mean Variance Optimization, and Black Litterman. \n",
    "\n",
    "Following a careful evaluation of wealthfront’s investment white paper and their methodology, I’ve come to the conclusion that creating a Black Litterman portfolio optimizer is well within my means.\n",
    "\n",
    "Every investor is looking to answer several questions: \n",
    "What assets to buy?\n",
    "When to buy said assets?\n",
    "How much of each asset to buy?\n",
    "When to sell the assets?\n",
    "\n",
    "A portfolio optimizer is mainly used to solve the third question; by optimizing the weights of each asset in the portfolio to maximize the sharpe ratio. However, what’s more about Black Litterman  is that it enables investors to combine their views regarding the performance of various assets with the market equilibrium in a manner that results in intuitive, diversified portfolios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build the optimizer, a more in depth understanding regarding Black Litterman model is required. Many of the material in this section is taken from the step by step guide to using Black Litterman. \n",
    "Black Litterman was created to address three main problems with the traditional mean variance optimization proposed as part of Harry Markowitz’s Modern Portfolio Theory: 1. Highly concentrated portfolios, 2. Input sensitivity, 3. Estimation error maximization. \n",
    "\n",
    "Mean variance optimized portfolios often yield extreme long short positions, and when constrained for only long positions, concentrates holdings in few assets. \n",
    "\n",
    "Input sensitivity refers to the fact that when using mean variance optimization, the resulting portfolio weights see massive changes when the expected returns of the assets change slightly. Black Litterman overcomes this by allowing investors to incorporate their own views to adjust the expected return. Finally, according to the guide and its cited report  has solved the problem of error maximization by spreading out the errors across the expected return vector. (I don’t fully understand this last error, so I’m going to simply assume it is correct.)\n",
    "\n",
    "Step by step guide :http://valuestockselector.com/investingarticles/BlackLitterman.pdf\n",
    "Report: https://www.jstor.org/stable/4479185?seq=1#page_scan_tab_contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Wealthfront whitepaper, Wealthfront has two different ETF combinations based on the investors preference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbols1= ['VTI', 'VEA', 'VWO', 'VIG', 'XLE', 'VTEB']\n",
    "symbols2= ['VTI', 'VEA', 'VWO', 'VIG', 'VNQ', 'LQD','EMB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify a start and end date for the period of historical pricing data for which we will be using to find the appropriate portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start= '2010-01-03'\n",
    "end= '2015-01-03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "returns_df1=load_returns_data(symbols1, start, end)\n",
    "returns_df2=load_returns_data(symbols2, start, end)\n",
    "print returns_df1.head()\n",
    "print returns_df1.tail()\n",
    "print returns_df2.head()\n",
    "print returns_df2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since VTEB doesn't have enough data to draw from as seen from the head and tail, we will be removing VTEB from the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "returns_df1=returns_df1.drop('VTEB', axis=1)\n",
    "print(returns_df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyze the first combination (returns_df1) before moving onto the second combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C1, R1= assetmeanvar(returns_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(pd.DataFrame(R1, columns=['Expected Returns'], index= returns_df1.columns).T)\n",
    "print('Covariance Matrix')\n",
    "display(pd.DataFrame(C1, columns=returns_df1.columns, index= returns_df1.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Market cap will have to be hardcoded, since Quantopian does not offer market cap information on ETFs. We will use Net Asset Value to represent market cap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Market_cap1={'VTI': 656960000000.000, 'VEA':104110000000.000, 'VWO': 89220000000.000, 'VIG':33550000000.000, 'XLE':16610000000.000}\n",
    "Market_cap2={'VTI': 656960000000.000, 'VEA':104110000000.000, 'VWO': 89220000000.000, 'VIG':33550000000.000, 'VNQ':34630000000.000, 'LQD':38460000000.000, 'EMB':12470000000.000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1= weightsbymarketcap(returns_df1, Market_cap1)\n",
    "W2= weightsbymarketcap(returns_df2, Market_cap2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We'e going to first calculate the weights generated by utilizing mean variance optimization, which works to maximizethe Sharpe ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Rf=0.015  #setting the risk free rate at 1.5% \n",
    "W1_mvo=solve_weights(R1, C1, Rf)\n",
    "display(pd.DataFrame([W1_mvo, W1], columns=returns_df1.columns, index= ['Weights_MVO', 'Weights_market_cap']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, in using MVO, it often only optimizes the portfolio with two allocations. This is problematic, and counter intuitive since in a real market setting, it may be imprudent to only have two allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m1,v1= mean_var(W1, R1, C1)\n",
    "lam1= lam(m1, v1, Rf)\n",
    "eqret1= eqret(C1, W1, lam1)\n",
    "display(pd.DataFrame([R1,eqret1], columns=returns_df1.columns, index=['Expected Returns', 'Equilibrium Returns']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1_eq=solve_weights(eqret1+Rf, C1, Rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Display the frontiers\n",
    "result1, front_mean1, front_var1= optimize_frontier(R1, C1, Rf)\n",
    "pairx1=[]\n",
    "pairy1=[]\n",
    "for i in range(len(R1)):\n",
    "    pairx1.append(C1[i][i]**0.5)\n",
    "    pairy1.append(R1[i])\n",
    "fig= plt.figure(figsize=(12, 24))\n",
    "ax= fig.add_subplot(2,1,1)\n",
    "ax.scatter(pairx1, pairy1)\n",
    "ax.plot(front_var1**0.5, front_mean1, label='MVO')\n",
    "plt.xlabel('variance')\n",
    "plt.ylabel('expected return')\n",
    "ax.legend(loc='upper left')\n",
    "for i, column in enumerate(returns_df1.columns):\n",
    "    ax.annotate(returns_df1.columns[i], (pairx1[i], pairy1[i]))\n",
    "ax.scatter(result1['tangent_variance']**0.5,result1['tangent_mean'])\n",
    "ax.annotate('tangent',(result1['tangent_variance']**0.5,result1['tangent_mean']))\n",
    "result1_eq, front_mean1_eq, front_var1_eq=optimize_frontier(eqret1+Rf, C1, Rf)\n",
    "pair_eqy1=[]\n",
    "for i in range(len(eqret1)):\n",
    "    pair_eqy1.append(eqret1[i]+Rf)\n",
    "pair_eqx1= pairx1\n",
    "ax.scatter(pair_eqx1, pair_eqy1, color= 'green')\n",
    "for i, column in enumerate(returns_df1.columns):\n",
    "    ax.annotate(returns_df1.columns[i], (pair_eqx1[i], pair_eqy1[i]))  \n",
    "ax.plot(front_var1_eq**0.5, front_mean1_eq, color= 'green', label='Black')\n",
    "ax.scatter(result1_eq['tangent_variance']**0.5, result1_eq['tangent_mean'],color='green')\n",
    "ax.annotate('tangent',(result1_eq['tangent_variance']**0.5, result1_eq['tangent_mean']))\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()\n",
    "display(pd.DataFrame([result1['weights'], W1, result1_eq['weights']], columns=returns_df1.columns, index= ['Weights_MVO', 'Weights_market_cap', 'Equilibrium Weights']))\n",
    "display(pd.DataFrame([[result1['tangent_mean'], result1['tangent_variance']**0.5],[result1_eq['tangent_mean'], result1_eq['tangent_variance']**0.5]], index=['MVO', 'Equilibrium'], columns=['expected_return', 'variance']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the graph, we can see how the expected(historical) returns form a efficient frontier (blue). Each point on the frontier represents a portfolio composition that minimizes variance at the specific expected return. The tangent portfolio on the blue curve is the MVO portfolio. We can also see how the curve is smoothed out through using reverse optimization with market weights in the first step of Black Litterman, and how the expected returns of each asset has shifted. It’s important to note each asset’s variance remains constant. The tangent point on the green curve represents the Equilibrium portfolio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the equilibrium weights is very close to the weights by market cap. This is due to the fact that the weights equilibrium returns are derived from the market weights, and as such the equilibrium weights are only adjusted with the risk free return, thus the two are nearly identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we will analyze the second combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C2, R2= assetmeanvar(returns_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(pd.DataFrame(R2, columns=['Expected Returns'], index= returns_df2.columns).T)\n",
    "print('Covariance Matrix')\n",
    "display(pd.DataFrame(C2, columns=returns_df2.columns, index= returns_df2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m2,v2= mean_var(W2, R2, C2)\n",
    "lam2= lam(m2, v2, Rf)\n",
    "eqret2= eqret(C2, W2, lam2)\n",
    "display(pd.DataFrame([R2,eqret2], columns=returns_df2.columns, index=['Expected Returns', 'Equilibrium Returns']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W2_mvo=solve_weights(R2, C2, Rf)\n",
    "display(pd.DataFrame([W2_mvo, W2], columns=returns_df2.columns, index= ['Weights_MVO', 'Weights_market_cap']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W2_eq=solve_weights(eqret2+Rf, C2, Rf)\n",
    "display(pd.DataFrame([W2_mvo, W2, W2_eq], columns=returns_df2.columns, index= ['Weights_MVO', 'Weights_market_cap', 'Equilibrium Weights']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Display the frontiers\n",
    "result2, front_mean2, front_var2= optimize_frontier(R2, C2, Rf)\n",
    "pairx2=[]\n",
    "pairy2=[]\n",
    "for i in range(len(R2)):\n",
    "    pairx2.append(C2[i][i]**0.5)\n",
    "    pairy2.append(R2[i])\n",
    "fig2= plt.figure(figsize=(12,24))\n",
    "ay= fig2.add_subplot(2,1,1)\n",
    "plt.xlabel('variance')\n",
    "plt.ylabel('expected return')\n",
    "ay.scatter(pairx2, pairy2)\n",
    "ay.plot(front_var2**0.5, front_mean2, label='MVO')\n",
    "for i, column in enumerate(returns_df2.columns):\n",
    "    ay.annotate(returns_df2.columns[i], (pairx2[i], pairy2[i]))\n",
    "ay.scatter(result2['tangent_variance']**0.5,result2['tangent_mean'])\n",
    "ay.annotate('tangent',(result2['tangent_variance']**0.5,result2['tangent_mean']))\n",
    "result2_eq, front_mean2_eq, front_var2_eq=optimize_frontier(eqret2+Rf, C2, Rf)\n",
    "pair_eqy2=[]\n",
    "for i in range(len(eqret2)):\n",
    "    pair_eqy2.append(eqret2[i]+Rf)\n",
    "pair_eqx2= pairx2\n",
    "ay.scatter(pair_eqx2, pair_eqy2, color= 'green')\n",
    "for i, column in enumerate(returns_df2.columns):\n",
    "    ay.annotate(returns_df2.columns[i], (pair_eqx2[i], pair_eqy2[i]))  \n",
    "ay.plot(front_var2_eq**0.5, front_mean2_eq, color= 'green', label='Black')\n",
    "ay.scatter(result2_eq['tangent_variance']**0.5, result2_eq['tangent_mean'],color='green')\n",
    "ay.annotate('tangent',(result2_eq['tangent_variance']**0.5, result2_eq['tangent_mean']))\n",
    "ay.legend(loc='upper left')\n",
    "plt.show()\n",
    "display(pd.DataFrame([result2['weights'], W2, result2_eq['weights']], columns=returns_df2.columns, index= ['Weights_MVO', 'Weights_market_cap', 'Equilibrium Weights']))\n",
    "display(pd.DataFrame([[result2['tangent_mean'], result2['tangent_variance']**0.5],[result2_eq['tangent_mean'], result2_eq['tangent_variance']**0.5]], index=['MVO', 'Equilibrium'], columns=['expected_return', 'variance']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this second portfolio combination. The tangent for the equilibrium return actually was able to produce a higher return than that of the original MVO combination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greatest advantage offered by Black Letterman's model is the ability to incorporate views about the varying assets against one another. The original MVO model requires you to incorporate views for every single asset, meaning, you have to input a specific expected return for each asset. This isn't very practical as traders and portfolio managers often have views of the assets in relation to each other. For example, I could believe that Google will outperform Apple, but I wouldn't know if Google can outperform the market and by how much.\n",
    "\n",
    "In order to incorporate views. We need to create a views matrix and an uncertainty matrix. For this section I will run the program for both combinations at the same time. The views I create are merely meant to show how it could affect the curves and the resulting tangent.  They do not reflect views that I hold for the market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First I need to create a list of views for each combination.\n",
    "\n",
    "#Portfolio 1 views\n",
    "views1 = [('VEA', '>','VWO', 0.03),\n",
    "        ('XLE','VTI', '>', 'VIG', 0.02),\n",
    "        ('VWO','<', 'XLE', 0.02)]\n",
    "#Portfolio 2 views\n",
    "views2 = [('VEA', '>','VWO', 0.03),\n",
    "        ('VTI','VEA','>', 'VIG', 0.01),\n",
    "        ('VWO','<', 'LQD', 0.02),\n",
    "         ('LQD','>', 'EMB', 0.02)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q1, P1=views_and_links(views1, Market_cap1, returns_df1.columns)\n",
    "print('the views matrix for portfolio 1')\n",
    "print(Q1)\n",
    "print('the links matrix for portfolio 1 ')\n",
    "print(P1)\n",
    "Q2, P2=views_and_links(views2, Market_cap2, returns_df2.columns)\n",
    "print('the views matrix for portfolio 2')\n",
    "print(Q2)\n",
    "print('the links matrix for portfolio 2')\n",
    "print(P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main additions I've made to the code is for the links matrix to take in more than 2 assets to be compaired, as seen from the second statement in both views, there are 3 assets. I can add as many views as I want for as many different combinations I choose. The way to read the views is that VEA will out perform VWO by 3%. \n",
    "\n",
    "The other thing is that I made sure for the links matrix to account for market caps when there are more two assets, so as to balance the effects of the views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tau= 0.025 #approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w1= uncertainty_m(tau, C1, P1) #uncertainty matrix\n",
    "print('uncertainty matrix for portfolio 1')\n",
    "print(w1)\n",
    "eqret1_adj=eqexcessret(tau, C1, P1, w1, Q1, eqret1)\n",
    "print('\\033[1m'+'equilibrium returns adjusted for views'+'\\033[0m')\n",
    "display(pd.DataFrame(eqret1_adj, index=returns_df1.columns, columns= ['Equilibrium Returns']).T)\n",
    "w2= uncertainty_m(tau, C2, P2) #uncertainty matrix\n",
    "print('uncertainty matrix for portfolio 2')\n",
    "print(w2)\n",
    "eqret2_adj=eqexcessret(tau, C2, P2, w2, Q2, eqret2)\n",
    "print('\\033[1m'+'equilibrium returns adjusted for views'+'\\033[0m')\n",
    "display(pd.DataFrame(eqret2_adj, index=returns_df2.columns, columns= ['Equilibrium Returns']).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once we have the new equilibrium returns, we will solve for the optimal portfolio weights, returns, and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Display the frontiers\n",
    "result1, front_mean1, front_var1= optimize_frontier(R1, C1, Rf)\n",
    "pairx1=[]\n",
    "pairy1=[]\n",
    "for i in range(len(R1)):\n",
    "    pairx1.append(C1[i][i]**0.5)\n",
    "    pairy1.append(R1[i])\n",
    "fig= plt.figure(figsize=(12, 24))\n",
    "ax= fig.add_subplot(2,1,1)\n",
    "ax.scatter(pairx1, pairy1)\n",
    "ax.plot(front_var1**0.5, front_mean1, label='MVO')\n",
    "plt.xlabel('variance')\n",
    "plt.ylabel('expected return')\n",
    "for i, column in enumerate(returns_df1.columns):\n",
    "    ax.annotate(returns_df1.columns[i], (pairx1[i], pairy1[i]))\n",
    "ax.scatter(result1['tangent_variance']**0.5,result1['tangent_mean'])\n",
    "ax.annotate('tangent',(result1['tangent_variance']**0.5,result1['tangent_mean']))\n",
    "result1_eq, front_mean1_eq, front_var1_eq=optimize_frontier(eqret1+Rf, C1, Rf)\n",
    "pair_eqy1=[]\n",
    "for i in range(len(eqret1)):\n",
    "    pair_eqy1.append(eqret1[i]+Rf)\n",
    "pair_eqx1= pairx1\n",
    "ax.scatter(pair_eqx1, pair_eqy1, color= 'green')\n",
    "for i, column in enumerate(returns_df1.columns):\n",
    "    ax.annotate(returns_df1.columns[i], (pair_eqx1[i], pair_eqy1[i]))  \n",
    "ax.plot(front_var1_eq**0.5, front_mean1_eq, color= 'green', label='Black')\n",
    "ax.scatter(result1_eq['tangent_variance']**0.5, result1_eq['tangent_mean'],color='green')\n",
    "ax.annotate('tangent',(result1_eq['tangent_variance']**0.5, result1_eq['tangent_mean']))\n",
    "\n",
    "result1_eqadj, front_mean1_eqadj, front_var1_eqadj= optimize_frontier(eqret1_adj+Rf, C1, Rf)\n",
    "pair_eqadjy1=[]\n",
    "for i in range(len(eqret1_adj)):\n",
    "    pair_eqadjy1.append(eqret1_adj[i]+Rf)\n",
    "pair_eqadjx1= pairx1\n",
    "ax.scatter(pair_eqadjx1, pair_eqadjy1, color= 'red')\n",
    "for i, column in enumerate(returns_df1.columns):\n",
    "    ax.annotate(returns_df1.columns[i], (pair_eqadjx1[i], pair_eqadjy1[i]))  \n",
    "ax.plot(front_var1_eqadj**0.5, front_mean1_eqadj, color= 'red', label='Black with views')\n",
    "ax.scatter(result1_eqadj['tangent_variance']**0.5, result1_eqadj['tangent_mean'],color='red')\n",
    "ax.annotate('tangent',(result1_eqadj['tangent_variance']**0.5, result1_eqadj['tangent_mean']))\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "result2_eqadj, front_mean2_eqadj, front_var2_eqadj= optimize_frontier(eqret2_adj+Rf, C2, Rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result2, front_mean2, front_var2= optimize_frontier(R2, C2, Rf)\n",
    "pairx2=[]\n",
    "pairy2=[]\n",
    "for i in range(len(R2)):\n",
    "    pairx2.append(C2[i][i]**0.5)\n",
    "    pairy2.append(R2[i])\n",
    "fig2= plt.figure(figsize=(12,24))\n",
    "ay= fig2.add_subplot(2,1,1)\n",
    "plt.xlabel('variance')\n",
    "plt.ylabel('expected return')\n",
    "ay.scatter(pairx2, pairy2)\n",
    "ay.plot(front_var2**0.5, front_mean2, label='MVO')\n",
    "for i, column in enumerate(returns_df2.columns):\n",
    "    ay.annotate(returns_df2.columns[i], (pairx2[i], pairy2[i]))\n",
    "ay.scatter(result2['tangent_variance']**0.5,result2['tangent_mean'])\n",
    "ay.annotate('tangent',(result2['tangent_variance']**0.5,result2['tangent_mean']))\n",
    "result2_eq, front_mean2_eq, front_var2_eq=optimize_frontier(eqret2+Rf, C2, Rf)\n",
    "pair_eqy2=[]\n",
    "for i in range(len(eqret2)):\n",
    "    pair_eqy2.append(eqret2[i]+Rf)\n",
    "pair_eqx2= pairx2\n",
    "ay.scatter(pair_eqx2, pair_eqy2, color= 'green')\n",
    "for i, column in enumerate(returns_df2.columns):\n",
    "    ay.annotate(returns_df2.columns[i], (pair_eqx2[i], pair_eqy2[i]))  \n",
    "ay.plot(front_var2_eq**0.5, front_mean2_eq, color= 'green', label='Black')\n",
    "ay.scatter(result2_eq['tangent_variance']**0.5, result2_eq['tangent_mean'],color='green')\n",
    "ay.annotate('tangent',(result2_eq['tangent_variance']**0.5, result2_eq['tangent_mean']))\n",
    "\n",
    "result2_eqadj, front_mean2_eqadj, front_var2_eqadj= optimize_frontier(eqret2_adj+Rf, C2, Rf)\n",
    "pair_eqadjy2=[]\n",
    "for i in range(len(eqret2_adj)):\n",
    "    pair_eqadjy2.append(eqret2_adj[i]+Rf)\n",
    "pair_eqadjx2= pairx2\n",
    "\n",
    "ay.scatter(pair_eqadjx2, pair_eqadjy2, color= 'red')\n",
    "for i, column in enumerate(returns_df2.columns):\n",
    "    ay.annotate(returns_df2.columns[i], (pair_eqadjx2[i], pair_eqadjy2[i]))  \n",
    "ay.plot(front_var2_eqadj**0.5, front_mean2_eqadj, color= 'red', label='Black with views')\n",
    "ay.scatter(result2_eqadj['tangent_variance']**0.5, result2_eqadj['tangent_mean'],color='red')\n",
    "ay.annotate('tangent',(result2_eqadj['tangent_variance']**0.5, result2_eqadj['tangent_mean']))\n",
    "ay.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in both graphs, the frontiers are shifted and each asset's position has also been shifted. Showing that the views do affect the expected returns and the frontier. And that shift also shifts the tangent portfolio composition and return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part is to evaluate how well the portfolio performs over the next 2 years based on the portfolio compositions suggested by the unadjusted portfolios. We use the unadjusted portfolios because we simply do not have the views to incorporate the market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_test= '2015-01-03'\n",
    "end_test= '2015-01-03'\n",
    "returns_df1_test=load_returns_data(returns_df1.columns, start, end)\n",
    "display(pd.DataFrame([W1_mvo, W1_eq], columns=returns_df1.columns, index= ['Weights_MVO','Equilibrium Weights']))\n",
    "returns_df2_test=load_returns_data(returns_df2.columns, start, end)\n",
    "display(pd.DataFrame([W2_mvo, W2_eq], columns=returns_df2.columns, index= ['Weights_MVO','Equilibrium Weights']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily_returns1_mvo_test=pd.DataFrame(np.dot(returns_df1_test, W1_mvo), index=returns_df1_test.index, columns=['Returns'])\n",
    "daily_returns1_eq_test=pd.DataFrame(np.dot(returns_df1_test, W1_eq), index=returns_df1_test.index, columns=['Returns'])\n",
    "daily_returns2_mvo_test=pd.DataFrame(np.dot(returns_df2_test, W2_mvo), index=returns_df2_test.index, columns=['Returns'])\n",
    "daily_returns2_eq_test=pd.DataFrame(np.dot(returns_df2_test, W2_eq), index=returns_df2_test.index, columns=['Returns'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig3=plt.figure(figsize=(12, 12))\n",
    "az=fig3.add_subplot(1,1,1)\n",
    "cum_returns1_mvo=((daily_returns1_mvo_test + 1).cumprod() - 1)\n",
    "az.plot(cum_returns1_mvo, color='blue', label='Portfrolio 1 MVO')\n",
    "cum_returns2_mvo=((daily_returns2_mvo_test + 1).cumprod() - 1)\n",
    "az.plot(cum_returns2_mvo, color='red', label='Portfolio 2 MVO')\n",
    "cum_returns1_eq=((daily_returns1_eq_test + 1).cumprod() - 1)\n",
    "az.plot(cum_returns1_eq, color='green', label='Portfolio 1 Black')\n",
    "cum_returns2_eq=((daily_returns2_eq_test + 1).cumprod() - 1)\n",
    "az.plot(cum_returns2_eq, color='yellow', label='Portfolio 2 Black')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Return')\n",
    "az.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based of off returns, Portfolio Composition 1 using MVO is the best portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mvo_test_returns1= cum_returns1_mvo['Returns'].iloc[-1]\n",
    "mvo_test_returns2= cum_returns2_mvo['Returns'].iloc[-1]\n",
    "eq_test_returns1= cum_returns1_eq['Returns'].iloc[-1]\n",
    "eq_test_returns2= cum_returns2_eq['Returns'].iloc[-1]\n",
    "print('Portfolio 1 MVO Returns: '+str(mvo_test_returns1))\n",
    "print('Portfolio 2 MVO Returns: '+str(mvo_test_returns2))\n",
    "print('Portfolio 1 Black Equilibrium Returns: '+str(eq_test_returns1))\n",
    "print('Portfolio 2 Black Equilibrium Returns: '+str(eq_test_returns2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of the std, Portfolio 2 using MOV is the best portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mvo_test_std1= daily_returns1_mvo_test['Returns'].std()\n",
    "mvo_test_std2= daily_returns2_mvo_test['Returns'].std()\n",
    "eq_test_std1= daily_returns1_eq_test['Returns'].std()\n",
    "eq_test_std2= daily_returns2_eq_test['Returns'].std()\n",
    "print('Portfolio 1 MVO Standard Deviation: '+str(mvo_test_std1))\n",
    "print('Portfolio 2 MVO Standard Deviation: '+str(mvo_test_std2))\n",
    "print('Portfolio 1 Black Equilibrium Standard Deviation: '+str(eq_test_std1))\n",
    "print('Portfolio 2 Black Equilibrium Standard Deviation: '+str(eq_test_std2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mvo_annual_return1=np.sqrt(1+mvo_test_returns1)-1\n",
    "mvo_annual_return2=np.sqrt(1+mvo_test_returns2)-1\n",
    "eq_annual_return1=np.sqrt(1+eq_test_returns1)-1\n",
    "eq_annual_return2=np.sqrt(1+eq_test_returns2)-1\n",
    "\n",
    "mvo_test_sharpe1= sharpe(mvo_annual_return1, mvo_test_std1, Rf)\n",
    "eq_test_sharpe1= sharpe(eq_annual_return1, eq_test_std1, Rf)\n",
    "mvo_test_sharpe2= sharpe(mvo_annual_return2, mvo_test_std2, Rf)\n",
    "eq_test_sharpe2= sharpe(eq_annual_return2, eq_test_std2, Rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Sharpe Ratio, Portfolio 2 using MVO has the highest sharpe ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection1={}\n",
    "for symbol in returns_df1_test.columns:\n",
    "    asset={}\n",
    "    asset['Cumulative Return']= ((returns_df1_test[symbol]+1).cumprod()-1)[-1]\n",
    "    asset['Annual Return']= np.sqrt(asset['Cumulative Return']+1)-1\n",
    "    asset['Standard Deviation']= returns_df1_test[symbol].std()\n",
    "    asset['Sharpe Ratio']= sharpe(asset['Annual Return'], asset['Standard Deviation'], Rf)\n",
    "    collection1[symbol]=asset \n",
    "coll1= pd.DataFrame(collection1, index=['Cumulative Return','Annual Return','Standard Deviation','Sharpe Ratio'], columns= returns_df1_test.columns )\n",
    "\n",
    "display(coll1)\n",
    "\n",
    "collection2={}\n",
    "for symbol in returns_df2_test.columns:\n",
    "    asset={}\n",
    "    asset['Cumulative Return']= ((returns_df2_test[symbol]+1).cumprod()-1)[-1]\n",
    "    asset['Annual Return']= np.sqrt(asset['Cumulative Return']+1)-1\n",
    "    asset['Standard Deviation']= returns_df2_test[symbol].std()\n",
    "    asset['Sharpe Ratio']= sharpe(asset['Annual Return'], asset['Standard Deviation'], Rf)\n",
    "    collection2[symbol]=asset \n",
    "coll2= pd.DataFrame(collection2, index=['Cumulative Return','Annual Return','Standard Deviation','Sharpe Ratio'], columns= returns_df2_test.columns)\n",
    "\n",
    "display(coll2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "portfoliomvo1= {'Cumulative Return': cum_returns1_mvo['Returns'].iloc[-1], 'Annual Return': mvo_annual_return1, 'Standard Deviation': mvo_test_std1, 'Sharpe Ratio': mvo_test_sharpe1 }\n",
    "portfoliomvo2= {'Cumulative Return': cum_returns2_mvo['Returns'].iloc[-1], 'Annual Return': mvo_annual_return2, 'Standard Deviation': mvo_test_std2, 'Sharpe Ratio': mvo_test_sharpe2 }\n",
    "portfolioeq1={'Cumulative Return': cum_returns1_eq['Returns'].iloc[-1], 'Annual Return': eq_annual_return1, 'Standard Deviation': eq_test_std1, 'Sharpe Ratio': eq_test_sharpe1 }\n",
    "portfolioeq2={'Cumulative Return': cum_returns2_eq['Returns'].iloc[-1], 'Annual Return': eq_annual_return2, 'Standard Deviation': eq_test_std2, 'Sharpe Ratio': eq_test_sharpe2 }\n",
    "portcolls={'Portfolio 1 MVO': portfoliomvo1, 'Portfolio 2 MVO': portfoliomvo2, 'Portfolio 1 Black': portfolioeq1, 'Portfolio 2 Black': portfolioeq2}\n",
    "portcolls_df= pd.DataFrame(portcolls, index=['Cumulative Return','Annual Return','Standard Deviation','Sharpe Ratio'], columns=['Portfolio 1 MVO','Portfolio 2 MVO', 'Portfolio 1 Black', 'Portfolio 2 Black'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project takes inspiration from the following sources:\n",
    "\n",
    "http://www.quantandfinancial.com/2013/08/black-litterman.html\n",
    "https://github.com/omartinsky/QuantAndFinancial/blob/master/black_litterman/black_litterman.ipynb\n",
    "A STEP-BY-STEP GUIDE TO THE BLACK-LITTERMAN MODEL  http://valuestockselector.com/investingarticles/BlackLitterman.pdf\n",
    "https://research.wealthfront.com/whitepapers/investment-methodology/\n",
    "\n",
    "Certain code snippets were edited or copied from \n",
    "https://github.com/omartinsky/QuantAndFinancial/blob/master/black_litterman/black_litterman.ipynb\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}